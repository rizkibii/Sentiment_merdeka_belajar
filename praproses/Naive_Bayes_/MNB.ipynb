{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import naive_bayes as nb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.dic_data_index = dict()\n",
    "        self.dic_data_by_class = dict()\n",
    "        self.dic_data_posterior = dict()\n",
    "        self.dic_data_prob_fitur = dict()\n",
    "        self.prior = dict()\n",
    "        self.class_ = 0\n",
    "        self.alpha = alpha\n",
    "#         self.count_doc_c = dict()\n",
    "\n",
    "    def train (self, X, y):\n",
    "#         print(X)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        self.X = vectorizer.fit_transform(X).A\n",
    "        self.fitur = vectorizer.get_feature_names()\n",
    "        self.len_fitur = len(self.fitur)\n",
    "        self.y = np.array(self.y)\n",
    "        self.class_ = sorted(set(y))\n",
    "        \n",
    "        self.prior_class = dict(zip(Counter(self.y).keys(), Counter(self.y).values()))\n",
    "        self.count_doc_c = self.prior_class.copy()\n",
    "        for key, value in self.prior_class.items():\n",
    "#             self.count_doc_c.update({key:len(self.y)})\n",
    "            self.prior_class.update({key:value/len(self.y)})\n",
    "        \n",
    "        #menghitung jumlah class\n",
    "        self.len_data = len(self.y)\n",
    "\n",
    "        #mencari index data untuk class tertentu\n",
    "#         self.dic_data_index = dict()\n",
    "        for i in self.class_:\n",
    "            isi_list = list()\n",
    "            for index, j in enumerate(self.y):\n",
    "        #         print(j)\n",
    "                if i == j:\n",
    "                    isi_list.append(index)\n",
    "            self.dic_data_index.update({i:isi_list})\n",
    "            \n",
    "#         self.dic_data_by_class = dict()\n",
    "        for key, value in self.dic_data_index.items():\n",
    "            self.dic_data_by_class.update({key:(self.X[value])})\n",
    "        # del self.dic_data_index\n",
    "        \n",
    "        self.count_fitur_c = dict()\n",
    "        self.dic_data_prob_fitur = dict()\n",
    "        for c, value in self.dic_data_by_class.items():\n",
    "            count_per_fitur = list()\n",
    "            prob_fitur = list()\n",
    "            sum_value  = value.sum()\n",
    "            for ix, f in enumerate(value.transpose()):\n",
    "                prob_fitur.append((f.sum()+self.alpha)/(sum_value+self.len_fitur))\n",
    "                count_per_fitur.append(f.sum())\n",
    "#                 prob_fitur.append((f.sum()/(len(value)+self.len_fitur)))\n",
    "#                 print(c,str(f.sum()), \"+\",str(self.alpha),\"/\", str(len(value)),\"+\", str(self.len_fitur), self.fitur[ix])\n",
    "            \n",
    "            _dict_count_fitur = dict(zip(self.fitur, count_per_doc))\n",
    "            bobot_fitur = dict(zip(self.fitur,prob_fitur))\n",
    "            \n",
    "            self.count_fitur_c.update({c:_dict_count_fitur})\n",
    "            self.dic_data_prob_fitur.update({c:bobot_fitur})\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         tostr = lamda str(x):x\n",
    "        if self.alpha <= 0:\n",
    "            raise Exception(\"alpha tidak boleh kurang dari atau sama dengan 0, alpha=\"+str(self.alpha)) \n",
    "        X = sorted(set(X.split()))\n",
    "#         try:\n",
    "        self.dict_predict = dict()\n",
    "        for c, value in self.dic_data_prob_fitur.items():\n",
    "#             print(len(value))\n",
    "\n",
    "            bobot_fitur = list()\n",
    "            for kata in X:\n",
    "#                 print(kata)\n",
    "                if kata in value:\n",
    "#                     print(value[kata])\n",
    "                    # print(c,kata,value[kata])\n",
    "                    bobot_fitur.append((self.count_fitur_c[c][kata]+self.alpha)/(self.count_doc_c[c]+self.len_fitur))\n",
    "                    print(\"P({}|{})=({}+{})/({}+{})\"\n",
    "                    .format(kata,c,self.count_fitur_c[c][kata],self.alpha,self.count_doc_c[c],self.len_fitur), end=\" = \")\n",
    "                    print((self.count_fitur_c[c][kata]+self.alpha)/(self.count_doc_c[c]+self.len_fitur))\n",
    "#  \n",
    "#                     print(\"P\"+\"(\"+kata+\"|\"+c+\")=\"+str((value[kata]+(self.alpha/(self.count_doc_c[c]+self.len_fitur)))))\n",
    "#                     \n",
    "            if len(bobot_fitur)>0:\n",
    "                post_prior = np.prod(bobot_fitur)*self.prior_class[c]\n",
    "#                 print(type(bobot_fitur))\n",
    "                bobot_fitur_ = [str(round(x, 3)) for x in bobot_fitur]\n",
    "                print(\"P({}|{}) = {} x {} = {}\".format(c,\"X\",self.prior_class[c], \" x \".join(bobot_fitur_), post_prior,20))\n",
    "#                 print(bobot_fitur)\n",
    "                self.dict_predict.update({c:post_prior})\n",
    "            print(\"\")\n",
    "        return max(self.dict_predict.items(), key=operator.itemgetter(1))[0]\n",
    "#         except:\n",
    "#             print(\"err404or\")\n",
    "#             return max(self.prior_class.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.read_excel(\"Book1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(foo.label.tolist())\n",
    "komentar = np.array(foo.komentar.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['obat herbal sehat murah',\n",
       "       'ingin putih glowing langsing ideal konsultasi langsung',\n",
       "       'mau punya kulit putih harga murah add putih kulit',\n",
       "       'mantap terima kasih pak',\n",
       "       'Tetap satu pintu Pak nanti Dana Siluman',\n",
       "       'Ini mantul mantap betul', 'sehat terus pak'], dtype='<U54')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.alpha = 1\n",
    "# model.train(komentar, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(herbal|non spam)=(0+1)/(4+30) = 0.029411764705882353\n",
      "P(langsing|non spam)=(0+1)/(4+30) = 0.029411764705882353\n",
      "P(obat|non spam)=(0+1)/(4+30) = 0.029411764705882353\n",
      "P(sehat|non spam)=(1+1)/(4+30) = 0.058823529411764705\n",
      "P(non spam|X) = 0.5714285714285714 x 0.029 x 0.029 x 0.029 x 0.059 = 8.552169086645446e-07\n",
      "\n",
      "P(herbal|spam)=(1+1)/(3+30) = 0.06060606060606061\n",
      "P(langsing|spam)=(1+1)/(3+30) = 0.06060606060606061\n",
      "P(obat|spam)=(1+1)/(3+30) = 0.06060606060606061\n",
      "P(sehat|spam)=(1+1)/(3+30) = 0.06060606060606061\n",
      "P(spam|X) = 0.42857142857142855 x 0.061 x 0.061 x 0.061 x 0.061 = 5.782124489863032e-06\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"obat herbal langsing sehat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separate(y, complement=False):\n",
    "    kelas = sorted(set(y))\n",
    "    \n",
    "    dict_index = dict()\n",
    "    for c in kelas:\n",
    "        index = list()\n",
    "        for ix, c_ in enumerate(y):\n",
    "            if complement==False and c==c_:\n",
    "                index.append(ix)\n",
    "            elif complement==True and c!=c_:\n",
    "                index.append(ix)\n",
    "        dict_index.update({c:index})\n",
    "    return dict_index\n",
    "\n",
    "def prior_(y):\n",
    "    unik = sorted(set(y))\n",
    "    dict_p = dict()\n",
    "    for c in unik:\n",
    "        count = y.tolist().count(c)\n",
    "        dict_p.update({c:count/len(y)})\n",
    "    return dict_p \n",
    "\n",
    "class MultinominalNaiveBayes:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.dict_nb = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.class_ = sorted(set(y))\n",
    "        self.prior = prior_(y)\n",
    "        index_data = data_separate(y)\n",
    "        self.an = len(X.A[0])\n",
    "        self.X = X\n",
    "        \n",
    "        self.dict_nb = dict()\n",
    "        for c in self.class_:\n",
    "            n_yi = np.sum(self.X[index_data[c]].A, axis=0)\n",
    "            n_y = self.X[index_data[c]].A.sum()\n",
    "            self.dict_nb.update({c:{}})\n",
    "            self.dict_nb[c][\"n_y\"] = n_y\n",
    "            self.dict_nb[c][\"n_yi\"] = n_yi\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        result = list()\n",
    "        for i in self.X.A:\n",
    "            list_pst = list()\n",
    "            for c in self.class_:\n",
    "                hat_theta = (self.dict_nb[c][\"n_yi\"]+self.alpha)/(self.dict_nb[c][\"n_y\"]+self.an)\n",
    "                posterior = np.prod(hat_theta**i)*self.prior[c]\n",
    "                list_pst.append(posterior)\n",
    "            result.append(self.class_[list_pst.index(max(list_pst))])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add', 'betul', 'dana', 'glowing', 'harga', 'herbal', 'ideal', 'ingin', 'ini', 'kasih', 'konsultasi', 'kulit', 'langsing', 'langsung', 'mantap', 'mantul', 'mau', 'murah', 'nanti', 'obat', 'pak', 'pintu', 'punya', 'putih', 'satu', 'sehat', 'siluman', 'terima', 'terus', 'tetap']\n",
      "(7, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as w\n",
    "# from sklearn.feature_extraction.text import CountVectorizer as w\n",
    "corpus = komentar\n",
    "vectorizer = w()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(['a','a','b','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spam', 'spam', 'spam', 'non spam', 'non spam', 'non spam', 'non spam']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = MultinominalNaiveBayes()\n",
    "model.train(X,label)\n",
    "print(model.alpha)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separate(y, complement=False):\n",
    "    kelas = sorted(set(y))\n",
    "    \n",
    "    dict_index = dict()\n",
    "    for c in kelas:\n",
    "        index = list()\n",
    "        for ix, c_ in enumerate(y):\n",
    "            if complement==False and c==c_:\n",
    "                index.append(ix)\n",
    "            elif complement==True and c!=c_:\n",
    "                index.append(ix)\n",
    "        dict_index.update({c:index})\n",
    "    return dict_index\n",
    "\n",
    "def prior_(y):\n",
    "    unik = sorted(set(y))\n",
    "    dict_p = dict()\n",
    "    for c in unik:\n",
    "        count = y.tolist().count(c)\n",
    "        dict_p.update({c:count/len(y)})\n",
    "    return dict_p \n",
    "\n",
    "class CNaiveBayes:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.dict_nb = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        self.class_ = sorted(set(y))\n",
    "        self.prior = prior_(y)\n",
    "        index_data = data_separate(y, complement=True) #complement=False\n",
    "        self.an = len(X.A[0])\n",
    "        self.X = X\n",
    "        \n",
    "        self.dict_nb = dict()\n",
    "        \n",
    "        for c in self.class_:\n",
    "            self.n_yi = np.sum(self.X[index_data[c]].A, axis=0)\n",
    "            self.n_y = self.X[index_data[c]].A.sum()\n",
    "            self.dict_nb.update({c:{}})\n",
    "            hat_theta = (self.n_yi+self.alpha)/(self.n_y+self.an)\n",
    "            w_ci = np.log(hat_theta)\n",
    "            \n",
    "            abs_sum_wci= np.sum(np.abs(w_ci))\n",
    "            norm_wci = w_ci/abs_sum_wci\n",
    "            self.dict_nb[c]['w_ci'] = norm_wci\n",
    "            self.dict_nb[c][\"n_y\"] = self.n_y\n",
    "            \n",
    "    def set_alpha(self, alpha =1):\n",
    "        self.alpha = alpha\n",
    "#         self.dict_nb = dict()\n",
    "        for c in self.class_:\n",
    "            self.dict_nb.update({c:{}})\n",
    "            hat_theta = (self.n_yi+self.alpha)/(self.n_y+self.an)\n",
    "            w_ci = np.log(hat_theta)\n",
    "            \n",
    "            abs_sum_wci= np.sum(np.abs(w_ci))\n",
    "            norm_wci = w_ci/abs_sum_wci\n",
    "            self.dict_nb[c]['w_ci'] = norm_wci\n",
    "#             self.dict_nb[c][\"n_y\"] = self.n_y\n",
    "            \n",
    "    def predict(self, X):\n",
    "        try:\n",
    "            self.X = X\n",
    "            result = list()\n",
    "            for i in self.X.A:\n",
    "                list_pst = list()\n",
    "                for c in self.class_:\n",
    "                    x = self.dict_nb[c][\"w_ci\"]\n",
    "                    posterior = np.sum(x*i)\n",
    "                    list_pst.append(posterior)\n",
    "                result.append(self.class_[list_pst.index(min(list_pst))])\n",
    "            return result\n",
    "        except:\n",
    "            return [self.class_[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spam']"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = CNaiveBayes()\n",
    "model.train(X,label)\n",
    "print(model.alpha)\n",
    "model.predict(X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_alpha(alpha =10)\n",
    "model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 1 1 0 0 1 2 1 1 0 0 1 1 0 0 0 0 1 3 0 0 0 0 0 0]\n",
      "16\n",
      "[0.0625 0.     0.     0.0625 0.0625 0.     0.0625 0.0625 0.     0.\n",
      " 0.0625 0.125  0.0625 0.0625 0.     0.     0.0625 0.0625 0.     0.\n",
      " 0.     0.     0.0625 0.1875 0.     0.     0.     0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "a= np.sum(X[[1,2]].A, axis=0)\n",
    "print(a)#/\n",
    "sum_ = (X[[1,2]].A.sum())\n",
    "print(sum_)\n",
    "print(a/sum_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 25])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([3,5])**np.array([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20)\t0.3971064382343257\n",
      "  (0, 9)\t0.5596754170004217\n",
      "  (0, 27)\t0.5596754170004217\n",
      "  (0, 14)\t0.4645786606417216\n",
      "  (1, 26)\t0.3921287418123664\n",
      "  (1, 2)\t0.3921287418123664\n",
      "  (1, 18)\t0.3921287418123664\n",
      "  (1, 21)\t0.3921287418123664\n",
      "  (1, 24)\t0.3921287418123664\n",
      "  (1, 29)\t0.3921287418123664\n",
      "  (1, 20)\t0.2782270638667323\n",
      "  (2, 1)\t0.5206467559864713\n",
      "  (2, 15)\t0.5206467559864713\n",
      "  (2, 8)\t0.5206467559864713\n",
      "  (2, 14)\t0.43218152024617124\n",
      "  (3, 28)\t0.6753558276781849\n",
      "  (3, 20)\t0.4791851475403949\n",
      "  (3, 25)\t0.5606033360566132\n"
     ]
    }
   ],
   "source": [
    "print(X[dataaa['non spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non spam': [3, 4, 5, 6], 'spam': [0, 1, 2]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0==0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0**1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1+5)/(2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5/(2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lufias/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separate(y, complement=False):\n",
    "    kelas = sorted(set(y))\n",
    "    \n",
    "    dict_index = dict()\n",
    "    for c in kelas:\n",
    "        index = list()\n",
    "        for ix, c_ in enumerate(y):\n",
    "            if complement==False and c==c_:\n",
    "                index.append(ix)\n",
    "            elif complement==True and c!=c_:\n",
    "                index.append(ix)\n",
    "        dict_index.update({c:index})\n",
    "    return dict_index\n",
    "\n",
    "def prior_(y):\n",
    "    unik = sorted(set(y))\n",
    "    dict_p = dict()\n",
    "    for c in unik:\n",
    "        count = y.tolist().count(c)\n",
    "        dict_p.update({c:count/len(y)})\n",
    "    return dict_p \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class NaiveBayesC:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.dict_nb = 0\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        self.X = vectorizer.fit_transform(X).A\n",
    "        \n",
    "        self.class_ = sorted(set(y))\n",
    "        self.prior = prior_(y)\n",
    "        index_data = data_separate(y)\n",
    "        self.an = len(X.A[0])\n",
    "        \n",
    "        self.dict_nb = dict()\n",
    "        for c in self.class_:\n",
    "            n_yi = np.sum(self.X[index_data[c]].A, axis=0)\n",
    "#             n_y = self.X[index_data[c]].A.sum()\n",
    "            n_y = len(self.X[index_data[c]].A)\n",
    "            self.dict_nb.update({c:{}})\n",
    "            self.dict_nb[c]['hat_theta'] = n_yi/(n_y+self.an)\n",
    "            self.dict_nb[c][\"n_y\"] = n_y\n",
    "            \n",
    "    def predict(self, X):\n",
    "        self.X = X\n",
    "        result = list()\n",
    "        for i in self.X.A:\n",
    "            list_pst = list()\n",
    "            for c in self.class_:\n",
    "                laplace = self.alpha/(self.dict_nb[c][\"n_y\"]+self.an)\n",
    "                x = self.dict_nb[c][\"hat_theta\"]+laplace\n",
    "                posterior = np.prod(x**i)*self.prior[c]\n",
    "                list_pst.append(posterior)\n",
    "            result.append(self.class_[list_pst.index(max(list_pst))])\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
