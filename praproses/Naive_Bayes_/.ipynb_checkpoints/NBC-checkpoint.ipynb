{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import naive_bayes as nb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.dic_data_index = dict()\n",
    "        self.dic_data_by_class = dict()\n",
    "        self.dic_data_posterior = dict()\n",
    "        self.dic_data_prob_fitur = dict()\n",
    "        self.prior = dict()\n",
    "        self.class_ = 0\n",
    "        self.alpha = alpha\n",
    "        self.list_post = list()\n",
    "#         self.count_doc_c = dict()\n",
    "\n",
    "    def train (self, X, y):\n",
    "#         print(X)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        self.X = vectorizer.fit_transform(X).A\n",
    "        self.fitur = vectorizer.get_feature_names()\n",
    "        self.len_fitur = len(self.fitur)\n",
    "        self.y = np.array(self.y)\n",
    "        self.class_ = sorted(set(y))\n",
    "        \n",
    "        self.prior_class = dict(zip(Counter(self.y).keys(), Counter(self.y).values()))\n",
    "        self.count_doc_c = self.prior_class.copy()\n",
    "        for key, value in self.prior_class.items():\n",
    "#             self.count_doc_c.update({key:len(self.y)})\n",
    "            self.prior_class.update({key:value/len(self.y)})\n",
    "        \n",
    "        #menghitung jumlah class\n",
    "        self.len_data = len(self.y)\n",
    "\n",
    "        #mencari index data untuk class tertentu\n",
    "#         self.dic_data_index = dict()\n",
    "        for i in self.class_:\n",
    "            isi_list = list()\n",
    "            for index, j in enumerate(self.y):\n",
    "        #         print(j)\n",
    "                if i == j:\n",
    "                    isi_list.append(index)\n",
    "            self.dic_data_index.update({i:isi_list})\n",
    "            \n",
    "#         self.dic_data_by_class = dict()\n",
    "        for key, value in self.dic_data_index.items():\n",
    "            self.dic_data_by_class.update({key:(self.X[value])})\n",
    "        # del self.dic_data_index\n",
    "        \n",
    "        self.count_fitur_c = dict()\n",
    "        self.dic_data_prob_fitur = dict()\n",
    "        for c, value in self.dic_data_by_class.items():\n",
    "            count_per_doc = list()\n",
    "            prob_fitur = list()\n",
    "            sum_value  = value.sum()\n",
    "            for ix, f in enumerate(value.transpose()):\n",
    "#                 prob_fitur.append((f.sum()+self.alpha)/(len(value)+self.len_fitur))\n",
    "                count_per_doc.append(f.sum())\n",
    "                prob_fitur.append((f.sum()/(len(value)+self.len_fitur)))\n",
    "#                 print(c,str(f.sum()), \"+\",str(self.alpha),\"/\", str(len(value)),\"+\", str(self.len_fitur), self.fitur[ix])\n",
    "            \n",
    "            _dict_count_fitur = dict(zip(self.fitur, count_per_doc))\n",
    "            bobot_fitur = dict(zip(self.fitur,prob_fitur))\n",
    "            \n",
    "            self.count_fitur_c.update({c:_dict_count_fitur})\n",
    "            self.dic_data_prob_fitur.update({c:bobot_fitur})\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "#         tostr = lamda str(x):x\n",
    "        if self.alpha <= 0:\n",
    "            raise Exception(\"alpha tidak boleh kurang dari atau sama dengan 0, alpha=\"+str(self.alpha)) \n",
    "        X = sorted(set(X.split()))\n",
    "#         try:\n",
    "        self.dict_predict = dict()\n",
    "        for c, value in self.dic_data_prob_fitur.items():\n",
    "#             print(len(value))\n",
    "\n",
    "            bobot_fitur = list()\n",
    "            for kata in X:\n",
    "#                 print(kata)\n",
    "                if kata in value:\n",
    "#                     print(value[kata])\n",
    "                    # print(c,kata,value[kata])\n",
    "                    bobot_fitur.append((self.count_fitur_c[c][kata]+self.alpha)/(self.count_doc_c[c]+self.len_fitur))\n",
    "                    self.list_post.append(\"P({}|{})=({}+{})/({}+{} = {})\"\n",
    "                    .format(kata,c,self.count_fitur_c[c][kata],self.alpha,self.count_doc_c[c],self.len_fitur, (self.count_fitur_c[c][kata]+self.alpha)/(self.count_doc_c[c]+self.len_fitur)))\n",
    "#                     print((self.count_fitur_c[c][kata]+self.alpha)/(self.count_doc_c[c]+self.len_fitur))\n",
    "#  \n",
    "#                     print(\"P\"+\"(\"+kata+\"|\"+c+\")=\"+str((value[kata]+(self.alpha/(self.count_doc_c[c]+self.len_fitur)))))\n",
    "#                     \n",
    "            if len(bobot_fitur)>0:\n",
    "                post_prior = np.prod(bobot_fitur)*self.prior_class[c]\n",
    "#                 print(type(bobot_fitur))\n",
    "                bobot_fitur_ = [str(round(x, 3)) for x in bobot_fitur]\n",
    "                print(\"P({}|{}) = {} x {} = {}\".format(c,\"X\",self.prior_class[c], \" x \".join(bobot_fitur_), post_prior,20))\n",
    "#                 print(bobot_fitur)\n",
    "                self.dict_predict.update({c:post_prior})\n",
    "            print(\"\")\n",
    "        return max(self.dict_predict.items(), key=operator.itemgetter(1))[0]\n",
    "#         except:\n",
    "#             print(\"err404or\")\n",
    "#             return max(self.prior_class.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = pd.read_excel(\"Book1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.array(foo.label.tolist())\n",
    "komentar = np.array(foo.komentar.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['obat herbal sehat murah',\n",
       "       'ingin putih glowing langsing ideal konsultasi langsung',\n",
       "       'mau punya kulit putih harga murah add putih kulit',\n",
       "       'mantap terima kasih pak',\n",
       "       'Tetap satu pintu Pak nanti Dana Siluman',\n",
       "       'Ini mantul mantap betul', 'sehat terus pak'], dtype='<U54')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.alpha = 1\n",
    "model.train(komentar, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(non spam|X) = 0.5714285714285714 x 0.029 x 0.029 x 0.029 x 0.059 = 8.552169086645446e-07\n",
      "\n",
      "P(spam|X) = 0.42857142857142855 x 0.061 x 0.061 x 0.061 x 0.061 = 5.782124489863032e-06\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"obat herbal langsing sehat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P(herbal|non spam)=(0+1)/(4+30 = 0.029411764705882353)',\n",
       " 'P(langsing|non spam)=(0+1)/(4+30 = 0.029411764705882353)',\n",
       " 'P(obat|non spam)=(0+1)/(4+30 = 0.029411764705882353)',\n",
       " 'P(sehat|non spam)=(1+1)/(4+30 = 0.058823529411764705)',\n",
       " 'P(herbal|spam)=(1+1)/(3+30 = 0.06060606060606061)',\n",
       " 'P(langsing|spam)=(1+1)/(3+30 = 0.06060606060606061)',\n",
       " 'P(obat|spam)=(1+1)/(3+30 = 0.06060606060606061)',\n",
       " 'P(sehat|spam)=(1+1)/(3+30 = 0.06060606060606061)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.list_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058823529411764705"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1+1)/(4+30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list()\n",
    "a.append(\"{} {} + {}\".format(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 + 3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
